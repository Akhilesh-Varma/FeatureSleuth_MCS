========================================================================================================================
                                        Experiment - FeatureMap Training
========================================================================================================================
This readme file explains about an experiment done by training a model on feature map.
========================================================================================================================

what did you try?
The experiment we tried is that, we trained a model(say m1) with simple architecture on an MNIST dataset and transfered
the weights of that model to a new model(say m2) with similar architecture and trained the new model, m2, on the feature
 maps extracted from the first layer(These feature maps are extracted using m1).
Note: The architecture of m2 is similar to m1 but not same because m2 will get the feature maps that has smaller size
than the original images size, so we have added padding to compansate that.


====================================================================

why did you try it?
When we train a model on a large number of images that has only the edges or curves (as feature maps extracted from
first layer contains only these) then it helps the model 'not' to overfit. In other terms this will act as a
regularization technique. So with this experiment would like tto see whether training the model on first layer of
feature maps helps in regularization i.e, will it give good results on the testing set after training the model
on the feature maps.

====================================================================

what did you expect by trying it?
From this experiment we expect an increment in the accuracy of the testing set. We are expecting that our model, with
same architecture, when trained on feature maps(taken from output of first layer) will outperform on the testing set
comparatively. That is we expect that testing accuracy will be more than that of the training accuracies.

====================================================================

did it happen the way you expected?
When we trained the model on the feature maps, we saw from first model, m1, that is trained on MNIST
dataset(with batch size of 512 and 10 epochs) of 60k images gave the accuracy of 99 on the training set but on the
testing set it was only 98. When we trained the new model, m2, by transfering the weights from m1 and training on
feature maps, we saw that the training and testing set accuracies were almost same however this time we saw a very
small improvement in the testing dataset compartively however the overall accuracy is decreased. This is because when
the model got trained on around 60000*15= 900,000 images and 60k images(which were trained with m1). 900,000 images
are only edges and curves and hence a decrease in accuracy is explainable.

====================================================================

what changes did you make to the models and/or to the data and/or to the training
procedure and/or to the evaluation? why?
We only used a simple architecture with 2 convolution layers and max pool layers after each of these convolution
layers and an an activation layer after Max pool layer. We then included two linear layers with an activation layer
in between them and finally a softmax.
	conv->maxpool->relu->conv->maxpool->relu->linear->relu->linear->softmax
In terms of architecture we have choosen this because, we don't want to include any sort of regularization in our
architecture and we would like to see if our approach of training the model on the feature maps helps itself in
regularization.
For evaluation, we have transfered the weights that we got from the new model, m2, and tested the model against the
MNIST test dataset.
We have choosen this technique of evaluation because when new model, m2, gets trained only on the feature maps that has
 less size (24,24) and hence we need to transfer the weights to the model, m1, to verify them on the original
 test dataset which has a size of (28,28). This image size matters because we write the no of units in linear layer
 according to the size of the image.

====================================================================

what experiments did you try in which you kept certain things fixed while varying others?
We experimented with different batch size - 64, 256, 512 etc. We wanted to see if the batch size makes any difference
when we are training with 900,000 images but we saw not a much difference.

====================================================================

summarize those results informally, and if possible present some graphs and/or tables

Results from training a model(m1) on 60k MNIST dataset:
*********************************
Accuracy after  1  epochs :  95.8233173076923
Accuracy after  2  epochs :  97.11538461538461
Accuracy after  3  epochs :  97.7363782051282
Accuracy after  4  epochs :  97.7764423076923
Accuracy after  5  epochs :  97.79647435897436
Accuracy after  6  epochs :  98.46754807692307
Accuracy after  7  epochs :  97.9266826923077
Accuracy after  8  epochs :  98.07692307692307
Accuracy after  9  epochs :  98.07692307692307
Accuracy after  10  epochs :  98.14703525641025
Accuracy on train set:
Accuracy after  10  epochs :  98.98612593383137
Accuracy on test set:
Accuracy after  10  epochs :  98.13701923076923


Results from training a model(m2) on 900,000 MNIST dataset:
*********************************
Accuracy after  1  epochs :  94.09950657894737
Accuracy after  2  epochs :  93.77055921052632
Accuracy after  3  epochs :  94.88075657894737
Accuracy after  4  epochs :  94.47985197368422
Accuracy after  5  epochs :  93.2360197368421
Accuracy after  6  epochs :  93.51356907894737
Accuracy after  7  epochs :  93.47245065789474
Accuracy after  8  epochs :  95.20970394736842
Accuracy after  9  epochs :  95.21998355263158
Accuracy after  10  epochs :  94.36677631578947
==================================
Final Accuracy Calculated on train set:
Accuracy on train set:  98.16038995726495
Final Accuracy Calculated on test set:
Accuracy after  10  epochs :  97.93379934210526
==================================
Final Accuracy Calculated on train set with featuremap trained:
Accuracy on train set:  94.24412393162393
Final Accuracy Calculated on test set with featuremap trained:
Accuracy on test set:  94.34621710526315


Future experiments:
We would like to experiment we would like to consider images of two numbers from the MNIST data
set and take all the feature maps obtained after each convolution layer. We will then pass these feature
maps as input images to a new model with the same architecture, and this new model is tested against
the original images taken from the MNIST data set for two numbers


Results after 3rd April

Params----Batch size: 256, sum_up_feature_channels=False, stop_counter_at = len(input_loader)/10

PyDev console: starting.
Python 3.8.13 (default, Mar 28 2022, 06:16:26)
[Clang 12.0.0 ] on darwin
runfile('/Users/rohinichandrala/Documents/Personal_Docs/Dalhousie/ML/Project_new/featuresleuth/TrainOnFeatureMaps.py', wdir='/Users/rohinichandrala/Documents/Personal_Docs/Dalhousie/ML/Project_new/featuresleuth')
  0%|          | 0/234 [00:00<?, ?it/s]/Users/rohinichandrala/Documents/Personal_Docs/Dalhousie/ML/Project_new/featuresleuth/Models/model.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  finput = self.softmax(finput)
Epoch[1/5]: 100%|██████████| 234/234 [00:43<00:00,  5.32it/s, accuracy=97, loss=0.0851, running_loss=97.7]
Epoch[2/5]: 100%|██████████| 234/234 [00:43<00:00,  5.39it/s, accuracy=97.7, loss=0.0818, running_loss=21.3]
Epoch[3/5]: 100%|██████████| 234/234 [00:43<00:00,  5.43it/s, accuracy=97.8, loss=0.0563, running_loss=15.8]
Epoch[4/5]: 100%|██████████| 234/234 [00:44<00:00,  5.31it/s, accuracy=98.1, loss=0.0333, running_loss=12.7]
Epoch[5/5]: 100%|██████████| 234/234 [00:43<00:00,  5.43it/s, accuracy=98.3, loss=0.0513, running_loss=10.7]
Accuracy on train set: 98.61444978632478
Accuracy on test set: 98.26722756410257
counter stopped at 24
  0%|          | 0/360 [00:00<?, ?it/s]/Users/rohinichandrala/Documents/Personal_Docs/Dalhousie/ML/Project_new/featuresleuth/Models/model.py:113: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  finput = self.softmax(finput)
Epoch[1/5]: 100%|██████████| 360/360 [01:01<00:00,  5.81it/s, accuracy=96.1, loss=0.286, running_loss=173]
Epoch[2/5]: 100%|██████████| 360/360 [01:00<00:00,  5.98it/s, accuracy=96.8, loss=0.21, running_loss=70.5]
Epoch[3/5]: 100%|██████████| 360/360 [01:00<00:00,  5.97it/s, accuracy=96.7, loss=0.0726, running_loss=52.9]
Epoch[4/5]: 100%|██████████| 360/360 [00:59<00:00,  6.03it/s, accuracy=96.5, loss=0.072, running_loss=42.5]
Epoch[5/5]: 100%|██████████| 360/360 [01:00<00:00,  5.95it/s, accuracy=96.8, loss=0.133, running_loss=35.7]
==================================
Final Accuracy Calculated on train set with feature map trained:
96.28739316239316
Final Accuracy Calculated on test set with feature map trained:
96.78485576923077


/Users/rohinichandrala/.conda/envs/featuresleuth/bin/python /Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevconsole.py --mode=client --port=61428
import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/rohinichandrala/Documents/Personal_Docs/Dalhousie/ML/Project_new/featuresleuth'])
PyDev console: starting.
Python 3.8.13 (default, Mar 28 2022, 06:16:26)
[Clang 12.0.0 ] on darwin
runfile('/Users/rohinichandrala/Documents/Personal_Docs/Dalhousie/ML/Project_new/featuresleuth/TrainOnFeatureMaps.py', wdir='/Users/rohinichandrala/Documents/Personal_Docs/Dalhousie/ML/Project_new/featuresleuth')
Experiment on Fashion MNIST Dataset
Batch-size: 256
Learning-rate: 0.01
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/mnist/train_data/FashionMNIST/raw/train-images-idx3-ubyte.gz
26422272it [00:07, 3711319.54it/s]
Extracting ./data/mnist/train_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/mnist/train_data/FashionMNIST/raw
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/mnist/train_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz
29696it [00:00, 180907.71it/s]
Extracting ./data/mnist/train_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/mnist/train_data/FashionMNIST/raw
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/mnist/train_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz
4422656it [00:01, 2966439.31it/s]
Extracting ./data/mnist/train_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/mnist/train_data/FashionMNIST/raw
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/mnist/train_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz
6144it [00:00, 11112463.90it/s]
Extracting ./data/mnist/train_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/mnist/train_data/FashionMNIST/raw
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/mnist/test_data/FashionMNIST/raw/train-images-idx3-ubyte.gz
26422272it [00:06, 4330441.06it/s]
Extracting ./data/mnist/test_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/mnist/test_data/FashionMNIST/raw
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/mnist/test_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz
29696it [00:00, 97823.64it/s]
Extracting ./data/mnist/test_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/mnist/test_data/FashionMNIST/raw
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/mnist/test_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz
4422656it [00:03, 1422361.62it/s]
Extracting ./data/mnist/test_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/mnist/test_data/FashionMNIST/raw
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/mnist/test_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz
6144it [00:00, 7914558.90it/s]
Extracting ./data/mnist/test_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/mnist/test_data/FashionMNIST/raw
  0%|          | 0/235 [00:00<?, ?it/s]/Users/rohinichandrala/Documents/Personal_Docs/Dalhousie/ML/Project_new/featuresleuth/Models/model.py:262: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.softmax(out)
Epoch[1/5]: 100%|██████████| 235/235 [01:47<00:00,  2.19it/s, accuracy=56, loss=1.32, running_loss=487]
Epoch[2/5]: 100%|██████████| 235/235 [01:50<00:00,  2.13it/s, accuracy=66.8, loss=0.796, running_loss=235]
Epoch[3/5]: 100%|██████████| 235/235 [01:51<00:00,  2.11it/s, accuracy=70.4, loss=0.714, running_loss=186]
Epoch[4/5]: 100%|██████████| 235/235 [01:50<00:00,  2.13it/s, accuracy=73.8, loss=0.654, running_loss=171]
Epoch[5/5]: 100%|██████████| 235/235 [01:49<00:00,  2.15it/s, accuracy=71.7, loss=0.559, running_loss=161]
Accuracy on train set: 72.89333333333333
Accuracy on test set: 71.69
Epoch[1/2]: 100%|██████████| 768/768 [05:40<00:00,  2.26it/s, accuracy=72.3, loss=1.25, running_loss=1.17e+3]
Epoch[2/2]: 100%|██████████| 768/768 [05:35<00:00,  2.29it/s, accuracy=71.8, loss=0.902, running_loss=758]
==================================
Final Accuracy Calculated on train set with feature map trained:
72.18833333333333
Final Accuracy Calculated on test set with feature map trained:
72.78
/Users/rohinichandrala/.conda/envs/featuresleuth/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
Process finished with exit code 0


